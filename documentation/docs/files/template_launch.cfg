# demo config for generic "hypot" experiment
[global]
# we use our fife_wrap as our wrapper...
wrapper = file:///${FIFE_UTILS_DIR}/libexec/fife_wrap
#
# This section has variables we use later in the file as %(name)s
#
experiment = hypot
group= %(experiment)s
# override on command line with proper sw. version
version = v_override_me
# override on command line with proper qualifiers
quals = override_me
# base output filename, etc., overridden in --stage sections (below)
basename= stage_override
# usually overridden on command line with poms
sam_dataset = override_me
fclfile = standard_%(basename)s_%(experiment)s.fcl
# experiment software to setup
exp_sw = %(experiment)scode
# for merge passes, override on command line
merge_streamname = merge_only
# for analysis stages, more option stuff
dest= /pnfs/%(experiment)s/scratch/users/$USER/output
tarfile = /%(experiment)s/app/users/$USER/Analysis.tgz
data_tier = root-tuple
stream_name = physics
#and for analysis_gen, triple slashed to get through the layers
nevents = 500
first_run = 1
first_subrun = \${PROCESS}
first_enum = \$\(\(PROCESS*%(nevents)s+1\)\)
first_event = %(first_run)s:%(first_subrun)s:%(first_enum)s

#
[env_pass]
#
# these become -e parameters to jobsub_submit
#
IFDH_DEBUG = 1
SAM_EXPERIMENT=%(experiment)s
SAM_GROUP=%(group)s
SAM_STATION=%(experiment)s

[submit]
#
# these become options to jobsub_submit
#
G = %(group)s
N = 5
#dataset =
resource-provides = usage_model=OPPORTUNISTIC,DEDICATED,OFFSITE
generate-email-summary = True
#expected-lifetime= 3h
#timeout= 2h
#OS = SL6
#disk= 10GB
#memory = 2000MB

[job_setup]
#
# these are options to fife_wrap about setting up the job environment,
# and main execution loop
#
debug = True
find_setups = False
source_1 = /cvmfs/%(experiment)s.opensciencegrid.org/products/%(experiment)s/setup_%(experiment)s.sh
setup_1 = %(exp_sw)s %(version)s -q %(quals)s
multifile = True
finally = echo "All done"
#getconfig = False
#ifdh_art= False

[sam_consumer]
#
# parameters to SAM / ifdh establishProcess
#
limit = 1
appvers = %(version)s
schema = gsiftp

[executable]
#
# parameters to main executable in job
#
name = lar
arg_1 = -c
arg_2 = %(fclfile)s
arg_3 = -o
arg_4 = %(basename)s_\\\\\\${fname##*/}.root
arg_5 = -T 
arg_6 = hist_%(basename)s_\\\\\\${fname##*/}.root
arg_7 = -n
arg_8 = %(nevents)s
arg_9 = -s
#arg_10= input_filename -- will be added by multifile loop...

[job_output]
#
# parameters to output handling section of fife_wrap
#
addoutput = %(basename)s*.root
rename = unique
dest = %(dest)s
declare_metadata = True
mtadata_extractor = sam_metadata_dumper
add_location = True

[job_output_1]
#
# parameters to output handling section of fife_wrap
#
addoutput = hist_%(basename)s*.root
rename = unique
dest = %(dest)s
declare_metadata = True
metadata_extractor = sam_metadata_dumper
[job_output_2]
#
# parameters to output handling section of fife_wrap
#
addoutput = *.[ol][ou][gt]
dest = %(dest)s
#
# now we have overides for each processing stage/job type 
#
# you may need to change the global.fclfile overrides for each
# stage to reflect your experiment's naming convention for .fcl files
#

[stage_gen]
# fake output dataset for POMS
job_output.add_to_dataset = _poms_task
job_output.dataset_exclude = hist*
# turn off -s flag
executable.arg_9 = 
global.fclfile = standard_genie_%(experiment)s.fcl
global.basename = gen
job_setup.multifile = False

[stage_g4]
global.basename = g4
submit.dataset = %(sam_dataset)s
# # if g4 only works onsite 
# submit.resource-provides= usage_model=OPPORTUNISTIC,DEDICATED
#
# # ...with extra cvmfs libraries:
# job_setup.prescript = export LD_LIBRARY_PATH=/cvmfs/nova.opensciencegrid.org/externals/library_shim/v03.03/NULL/lib/sl6:$LD_LIBRARY_PATH

[stage_detsim]
global.basename = detsim
submit.dataset = %(sam_dataset)s

[stage_reco]
global.basename = reco
global.fclfile = standard_reco_%(experiment)s_basic.fcl
submit.dataset = %(sam_dataset)s

[stage_anatree]
global.basename = anatree
submit.dataset = %(sam_dataset)s

[stage_split]
global.basename = split
executable.arg_3=
executable.arg_4=
submit.dataset = %(sam_dataset)s

[stage_merge]
global.basename = %(merge_streamname)s
submit.dataset = %(sam_dataset)s

[stage_analyze]
global.basename = ana
submit.dataset = %(sam_dataset)s
job_setup.setup_local = True
submit.tar_file_name = %(tarfile)s 

[stage_analysis_reco]
global.basename = ana_reco
submit.dataset = %(sam_dataset)s
job_setup.setup_local = True
submit.tar_file_name = %(tarfile)s 

[stage_analysis_ana]
global.basename = ana
submit.dataset = %(sam_dataset)s
job_setup.setup_local = True
submit.tar_file_name = %(tarfile)s 

[stage_analysis_gen]
global.basename = ana_gen
job_setup.setup_local = True
submit.tar_file_name= %(tarfile)s 
job_setup.multifile = False
#blank out -s input flag
executable.arg_9 = --estart
executable.arg_10 = %(first_event)s
job_output.add_to_dataset==_poms_task

[stage_analysis_ana_output]
global.basename = ana
submit.dataset = %(sam_dataset)s
job_setup.setup_local = True
submit.tar_file_name = %(tarfile)s 
#blank out -o filename args
executable.arg_3 = 
executable.arg_4 =

[stage_parent_demo]
parent = analysis_ana_output
